{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "GeForce RTX 3080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorforce import Agent, Environment, Runner\n",
    "\n",
    "\n",
    "class MultiactorEnvironment(Environment):\n",
    "    \"\"\"\n",
    "    Example multi-actor environment, illustrating best-practice implementation pattern.\n",
    "    State space: position in [0, 10].\n",
    "    Action space: movement in {-1, 0, 1}.\n",
    "    Random start in [3, 7].\n",
    "    Actor 1 perspective as is, actor 2 perspective mirrored.\n",
    "    Positive reward for being closer to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def states(self):\n",
    "        return dict(type='int', num_values=11)\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(type='int', num_values=3)\n",
    "\n",
    "    def num_actors(self):\n",
    "        return 2  # Indicates that environment has multiple actors\n",
    "\n",
    "    def reset(self):\n",
    "        # Always for multi-actor environments: initialize parallel indices\n",
    "        self._parallel_indices = np.arange(self.num_actors())\n",
    "\n",
    "        # Single shared environment logic, plus per-actor perspective\n",
    "        self._states = 3 + np.random.randint(5)\n",
    "        self.second_actor = True\n",
    "        states = np.stack([self._states, 10 - self._states], axis=0)\n",
    "\n",
    "        # Always for multi-actor environments: return per-actor values\n",
    "        return self._parallel_indices.copy(), states\n",
    "\n",
    "    def execute(self, actions):\n",
    "        # Single shared environment logic, plus per-actor perspective\n",
    "        if self.second_actor:\n",
    "            self.second_actor = self.second_actor and not (np.random.random_sample() < 0.1)\n",
    "            terminal = np.stack([False, not self.second_actor], axis=0)\n",
    "            delta = (actions[0] - 1) - (actions[1] - 1)\n",
    "            self._states = np.clip(self._states + delta, a_min=0, a_max=10)\n",
    "            states = np.stack([self._states, 10 - self._states], axis=0)\n",
    "        else:\n",
    "            terminal = np.stack([False], axis=0)\n",
    "            delta = (actions[0] - 1)\n",
    "            self._states = np.clip(self._states + delta, a_min=0, a_max=10)\n",
    "            states = np.stack([self._states], axis=0)\n",
    "        reward = (states - 5.0) / 5.0\n",
    "\n",
    "        # Always for multi-actor environments: update parallel indices, and return per-actor values\n",
    "        self._parallel_indices = self._parallel_indices[~terminal]\n",
    "        return self._parallel_indices.copy(), states, terminal, reward\n",
    "\n",
    "\n",
    "environment = Environment.create(\n",
    "    environment=MultiactorEnvironment, max_episode_timesteps=10000\n",
    ")\n",
    "\n",
    "\n",
    "agent = Agent.create(\n",
    "    agent='ppo',\n",
    "    environment=environment,  # alternatively: states, actions, (max_episode_timesteps)\n",
    "    network={\"type\": \"auto\", \"rnn\": False},\n",
    "    use_beta_distribution=False,\n",
    "    memory=\"minimum\",\n",
    "    batch_size=12,\n",
    "    update_frequency=1,\n",
    "    learning_rate=0.001813150053725916,\n",
    "    multi_step=5,\n",
    "    subsampling_fraction=0.9131375430837279,\n",
    "    likelihood_ratio_clipping=0.09955676846552193,\n",
    "    discount=0.9985351346308641,\n",
    "    return_processing=None,\n",
    "    advantage_processing=None,\n",
    "    predict_terminal_values=False,\n",
    "    reward_processing=None,\n",
    "    baseline={\"type\": \"auto\", \"rnn\":False},\n",
    "    baseline_optimizer={\"optimizer\": \"adam\", \"learning_rate\": 0.003670157218888348, \"multi_step\":10},\n",
    "    l2_regularization=0.0,\n",
    "    entropy_regularization=0.0011393096635237982,\n",
    "    state_preprocessing=\"linear_normalization\",\n",
    "    exploration=0.0,\n",
    "    variable_noise=0.0\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1256/1567501050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m runner = Runner(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# agent='json/ppo.json',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMultiactorEnvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorforce/execution/runner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent, environment, max_episode_timesteps, num_parallel, environments, evaluation, remote, blocking, host, port)\u001b[0m\n\u001b[1;32m    208\u001b[0m             )\n\u001b[1;32m    209\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnum_parallel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             self.agent = Agent.create(\n\u001b[0m\u001b[1;32m    211\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mparallel_interactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_parallel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'parallel_interactions'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runner = Runner(\n",
    "    # agent='json/ppo.json',\n",
    "    agent=agent,\n",
    "    environment=MultiactorEnvironment,\n",
    "    max_episode_timesteps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80440a8149934178a72e99f403b2ba27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episodes:   0%|          | 0/100 [00:00, return=0.00, ts/ep=0, sec/ep=0.00, ms/ts=0.0, agent=0.0%]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "runner.run(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'agent',\n",
       " 'close',\n",
       " 'environments',\n",
       " 'evaluation',\n",
       " 'handle_act',\n",
       " 'handle_act_evaluation',\n",
       " 'handle_act_joint',\n",
       " 'handle_observe',\n",
       " 'handle_observe_evaluation',\n",
       " 'handle_observe_joint',\n",
       " 'handle_terminal',\n",
       " 'handle_terminal_evaluation',\n",
       " 'is_agent_external',\n",
       " 'is_environment_external',\n",
       " 'is_environment_remote',\n",
       " 'num_vectorized',\n",
       " 'run']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_is_agent',\n",
       " '_process_states_input',\n",
       " 'act',\n",
       " 'actions_spec',\n",
       " 'auxiliaries_spec',\n",
       " 'close',\n",
       " 'config',\n",
       " 'create',\n",
       " 'deterministic_spec',\n",
       " 'episodes',\n",
       " 'experience',\n",
       " 'fn_act',\n",
       " 'get_architecture',\n",
       " 'get_specification',\n",
       " 'initial_internals',\n",
       " 'initialize',\n",
       " 'internals_spec',\n",
       " 'is_initialized',\n",
       " 'load',\n",
       " 'max_episode_timesteps',\n",
       " 'model',\n",
       " 'observe',\n",
       " 'parallel_interactions',\n",
       " 'parallel_spec',\n",
       " 'pretrain',\n",
       " 'recorder',\n",
       " 'reset',\n",
       " 'restore',\n",
       " 'reward_buffer',\n",
       " 'reward_spec',\n",
       " 'save',\n",
       " 'spec',\n",
       " 'states_spec',\n",
       " 'terminal_buffer',\n",
       " 'terminal_spec',\n",
       " 'timestep_completed',\n",
       " 'timestep_counter',\n",
       " 'timesteps',\n",
       " 'tracked_tensors',\n",
       " 'update',\n",
       " 'updates']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(runner.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.agent.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'int', 'num_values': 11}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.environments[0].states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
